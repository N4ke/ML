{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3f7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import nlpaug.augmenter.word as naw\n",
    "import preprocessor as p\n",
    "import multiprocessing\n",
    "from contextlib import contextmanager\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, MarianTokenizer, MarianMTModel\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ddc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d567dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['keyword', 'text', 'target']]\n",
    "test_df = test_df[['keyword', 'text']]\n",
    "\n",
    "train_df['keyword'] = train_df['keyword'].fillna('None')\n",
    "test_df['keyword'] = test_df['keyword'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ea59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_clean(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.HASHTAG, p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: extend_clean(p.clean(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a54c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='xlm-roberta-base',\n",
    "#     action='substitute'\n",
    "# )\n",
    "# aug_rows = []\n",
    "\n",
    "# for idx, row in train_df.iterrows():\n",
    "#     aug_text = aug.augment(row['text'])\n",
    "#     new_row = {\n",
    "#         'keyword': row['keyword'],\n",
    "#         'text': aug_text,\n",
    "#         'target': row['target']\n",
    "#     }\n",
    "#     aug_rows.append(new_row)\n",
    "\n",
    "# aug_df = pd.DataFrame(aug_rows)\n",
    "# new_df = pd.concat([train_df, aug_df], ignore_index=True)\n",
    "# new_df.to_csv('./data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/952 [00:10<2:38:53, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout on batch 0, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/952 [00:20<2:38:36, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout on batch 16, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/952 [00:30<2:38:23, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout on batch 32, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/952 [00:40<3:31:11, 13.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m translated_texts\n\u001b[0;32m     29\u001b[0m translated_df \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 30\u001b[0m translated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslated_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mde\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m translated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m translate(translated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     32\u001b[0m new_df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([train_df, translated_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(texts, src_lang, out_lang, batch_size, timeout)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     result \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mapply_async(translate_batch, (batch, src_lang, out_lang))\n\u001b[1;32m---> 19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     translated_texts\u001b[38;5;241m.\u001b[39mextend(outputs)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mTimeoutError:\n",
      "File \u001b[1;32mc:\\Users\\justt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[0;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\justt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\justt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:634\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    632\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 634\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\justt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:338\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 338\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def translate_batch(batch, src_lang, out_lang):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{out_lang}'\n",
    "    model = MarianMTModel.from_pretrained(model_name).to('cuda')\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        translated = model.generate(**inputs, early_stopping=True)\n",
    "    outputs = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return outputs\n",
    "\n",
    "def translate(texts, src_lang, out_lang, batch_size=32, timeout=10):\n",
    "    translated_texts = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        with multiprocessing.get_context(\"spawn\").Pool(1) as pool:\n",
    "            try:\n",
    "                result = pool.apply_async(translate_batch, (batch, src_lang, out_lang))\n",
    "                outputs = result.get(timeout=timeout)\n",
    "                translated_texts.extend(outputs)\n",
    "            except multiprocessing.TimeoutError:\n",
    "                print(f\"Timeout on batch {i / batch_size}, skipping...\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error on batch {i}: {e}\")\n",
    "                continue\n",
    "    return translated_texts\n",
    "\n",
    "translated_df = train_df.copy()\n",
    "translated_df['text'] = translate(translated_df['text'].tolist(), 'en', 'de', batch_size=16)\n",
    "translated_df['text'] = translate(translated_df['text'].tolist(), 'de', 'en', batch_size=16)\n",
    "new_df2 = pd.concat([train_df, translated_df], ignore_index=True)\n",
    "new_df2.to_csv('./data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c306a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128, is_test=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        keyword = str(self.df.iloc[index]['keyword'])\n",
    "        text = str(self.df.iloc[index]['text'])\n",
    "        \n",
    "        comb_text = keyword + \" : \" + text\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            comb_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        \n",
    "        if not self.is_test and 'target' in self.df.columns:\n",
    "            labels = torch.tensor(self.df.iloc[index]['target'], dtype=torch.long)\n",
    "            return inputs, labels\n",
    "        else:\n",
    "            return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8efeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "train_val_dataset = CustomDataset(train_df, tokenizer)\n",
    "test_dataset = CustomDataset(test_df, tokenizer, is_test=True)\n",
    "\n",
    "train_length = int(len(train_val_dataset) * 0.8)\n",
    "val_length = len(train_val_dataset) - train_length\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_length, val_length])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "236b8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer, \n",
    "        scheduler,\n",
    "        num_epochs=10,\n",
    "        criterion=None,\n",
    "        device=None\n",
    "    ):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    scaler = torch.GradScaler(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        tqdm_train = tqdm(train_dataloader, desc=f\"Training epoch {epoch + 1}: \", leave=False)\n",
    "        \n",
    "        for inputs, labels in tqdm_train:\n",
    "            if isinstance(inputs, dict):\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            tqdm_train.set_postfix(loss=loss.item())\n",
    "        \n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        train_f1 = metrics.f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        tqdm_val = tqdm(val_dataloader, desc=f\"Validation epoch {epoch + 1}: \", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm_val:\n",
    "                if isinstance(inputs, dict):\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                with torch.autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                \n",
    "                tqdm_val.set_postfix(loss=loss.item())\n",
    "        \n",
    "        val_loss = running_loss / len(val_dataloader)\n",
    "        val_f1 = metrics.f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print(f\"--- Epoch {epoch + 1}/{num_epochs}\\n\"\n",
    "              f\"Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\\n\"\n",
    "              f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b640b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "743b8ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/12\n",
      "Train F1: 0.7224, Val F1: 0.7926\n",
      "Train loss: 0.5638, Val loss: 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 2/12\n",
      "Train F1: 0.7785, Val F1: 0.7988\n",
      "Train loss: 0.4913, Val loss: 0.4492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 3/12\n",
      "Train F1: 0.7806, Val F1: 0.8018\n",
      "Train loss: 0.4885, Val loss: 0.4449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 4/12\n",
      "Train F1: 0.7817, Val F1: 0.7854\n",
      "Train loss: 0.4798, Val loss: 0.4603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 5/12\n",
      "Train F1: 0.7842, Val F1: 0.8036\n",
      "Train loss: 0.4731, Val loss: 0.4368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 6/12\n",
      "Train F1: 0.7900, Val F1: 0.8031\n",
      "Train loss: 0.4674, Val loss: 0.4344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 7/12\n",
      "Train F1: 0.7927, Val F1: 0.8035\n",
      "Train loss: 0.4587, Val loss: 0.4369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 8/12\n",
      "Train F1: 0.7920, Val F1: 0.8057\n",
      "Train loss: 0.4630, Val loss: 0.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 9/12\n",
      "Train F1: 0.7923, Val F1: 0.8081\n",
      "Train loss: 0.4601, Val loss: 0.4310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 10/12\n",
      "Train F1: 0.7969, Val F1: 0.8091\n",
      "Train loss: 0.4509, Val loss: 0.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 11/12\n",
      "Train F1: 0.7971, Val F1: 0.8065\n",
      "Train loss: 0.4543, Val loss: 0.4301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 12/12\n",
      "Train F1: 0.7970, Val F1: 0.8072\n",
      "Train loss: 0.4509, Val loss: 0.4291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5b5b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in test_dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e163bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7b4794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8068"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del scheduler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70c0a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv('./sample_submission.csv')\n",
    "test_csv['target'] = predictions\n",
    "test_csv.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
