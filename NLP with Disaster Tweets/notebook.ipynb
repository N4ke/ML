{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca3f7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import nlpaug.augmenter.word as naw\n",
    "import preprocessor as p\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, MarianTokenizer, MarianMTModel\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4ddc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d567dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['keyword', 'text', 'target']]\n",
    "test_df = test_df[['keyword', 'text']]\n",
    "\n",
    "train_df['keyword'] = train_df['keyword'].fillna('None')\n",
    "test_df['keyword'] = test_df['keyword'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03ea59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_clean(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\b(?:gt)+\\b', '', text)\n",
    "    return text\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.HASHTAG, p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: extend_clean(p.clean(x).lower()))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: extend_clean(p.clean(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a54c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='xlm-roberta-base',\n",
    "#     action='substitute'\n",
    "# )\n",
    "# aug_rows = []\n",
    "\n",
    "# for idx, row in train_df.iterrows():\n",
    "#     aug_text = aug.augment(row['text'])\n",
    "#     new_row = {\n",
    "#         'keyword': row['keyword'],\n",
    "#         'text': aug_text,\n",
    "#         'target': row['target']\n",
    "#     }\n",
    "#     aug_rows.append(new_row)\n",
    "\n",
    "# aug_df = pd.DataFrame(aug_rows)\n",
    "# new_df = pd.concat([train_df, aug_df], ignore_index=True)\n",
    "# new_df.to_csv('./data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [05:55<00:00,  1.34it/s]\n",
      "100%|██████████| 476/476 [15:20<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# def translate(texts, src_lang, out_lang, batch_size=32):\n",
    "#     model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{out_lang}'\n",
    "#     model = MarianMTModel.from_pretrained(model_name).to('cuda')\n",
    "#     tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "     \n",
    "#     translated_texts = []\n",
    "#     for i in tqdm(range(0, len(texts), batch_size)):\n",
    "#         batch = texts[i:i+batch_size]\n",
    "#         inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "#         with torch.no_grad():\n",
    "#             translated = model.generate(**inputs, early_stopping=True, max_length=128)\n",
    "#         outputs = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "#         translated_texts.extend(outputs)\n",
    "        \n",
    "#     return translated_texts\n",
    "\n",
    "# translated_df = train_df.copy()\n",
    "# translated_df['text'] = translate(translated_df['text'].tolist(), 'en', 'de', batch_size=32)\n",
    "# translated_df['text'] = translate(translated_df['text'].tolist(), 'de', 'en', batch_size=32)\n",
    "# new_df2 = pd.concat([train_df, translated_df], ignore_index=True)\n",
    "# new_df2.to_csv('./data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c306a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128, is_test=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        keyword = str(self.df.iloc[index]['keyword'])\n",
    "        text = str(self.df.iloc[index]['text'])\n",
    "        \n",
    "        comb_text = keyword + \" : \" + text\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            comb_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        \n",
    "        if not self.is_test and 'target' in self.df.columns:\n",
    "            labels = torch.tensor(self.df.iloc[index]['target'], dtype=torch.long)\n",
    "            return inputs, labels\n",
    "        else:\n",
    "            return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff8efeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "train_val_dataset = CustomDataset(train_df, tokenizer)\n",
    "test_dataset = CustomDataset(test_df, tokenizer, is_test=True)\n",
    "\n",
    "train_length = int(len(train_val_dataset) * 0.8)\n",
    "val_length = len(train_val_dataset) - train_length\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_length, val_length])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "236b8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer, \n",
    "        scheduler,\n",
    "        num_epochs=10,\n",
    "        criterion=None,\n",
    "        device=None\n",
    "    ):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    scaler = torch.GradScaler(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        tqdm_train = tqdm(train_dataloader, desc=f\"Training epoch {epoch + 1}: \", leave=False)\n",
    "        \n",
    "        for inputs, labels in tqdm_train:\n",
    "            if isinstance(inputs, dict):\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            else:\n",
    "                inputs = inputs.to(device)\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            tqdm_train.set_postfix(loss=loss.item())\n",
    "        \n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        train_f1 = metrics.f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        tqdm_val = tqdm(val_dataloader, desc=f\"Validation epoch {epoch + 1}: \", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm_val:\n",
    "                if isinstance(inputs, dict):\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                with torch.autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                \n",
    "                tqdm_val.set_postfix(loss=loss.item())\n",
    "        \n",
    "        val_loss = running_loss / len(val_dataloader)\n",
    "        val_f1 = metrics.f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print(f\"--- Epoch {epoch + 1}/{num_epochs}\\n\"\n",
    "              f\"Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\\n\"\n",
    "              f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29b640b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "743b8ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/6\n",
      "Train F1: 0.7203, Val F1: 0.7923\n",
      "Train loss: 0.5721, Val loss: 0.4886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 2/6\n",
      "Train F1: 0.7841, Val F1: 0.8079\n",
      "Train loss: 0.4971, Val loss: 0.4466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 3/6\n",
      "Train F1: 0.7904, Val F1: 0.8108\n",
      "Train loss: 0.4801, Val loss: 0.4342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 4/6\n",
      "Train F1: 0.7946, Val F1: 0.8111\n",
      "Train loss: 0.4660, Val loss: 0.4279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 5/6\n",
      "Train F1: 0.7961, Val F1: 0.8118\n",
      "Train loss: 0.4637, Val loss: 0.4262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 6/6\n",
      "Train F1: 0.7964, Val F1: 0.8113\n",
      "Train loss: 0.4634, Val loss: 0.4258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5b5b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in test_dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e163bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da7b4794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del scheduler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70c0a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv('./sample_submission.csv')\n",
    "test_csv['target'] = predictions\n",
    "test_csv.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
